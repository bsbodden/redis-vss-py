{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Similarity Search with Redis\n",
    "### Exploring Redis as a Vector Database\n",
    "\n",
    "with Brian Sam-Bodden \n",
    "\n",
    "![redis](./images/redis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The \"Unstructured Data\" Problem\n",
    "\n",
    "- The **balanced** of data has changed radically... \n",
    "- **~80%** of the data generated by organizations is **Unstructured**<sup>(IDC report, 2020)</sup>\n",
    "- This percentage is estimated to keep growing <sup>(with CAGR of 36.5% between 2020 and 2025)</sup>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But what is \"Unstructured\" Data?\n",
    "\n",
    "- Data that does not conform to a **pre-defined** data model\n",
    "- Data that can not be easily **\"indexed\"** by a search engine\n",
    "- Data is typically **high-dimensional** and **semantically** rich\n",
    "- Examples include **images**, **videos**, **free-form text**, and **audio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![data pyramid](./images/data-balance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with Unstructured Data\n",
    "\n",
    "- Unstructured data must be **transformed**\n",
    "- To deal with the **high-dimensional** nature we extract **\"features\"**\n",
    "- Traditional extraction techniques included **labelling**, **tagging**, and **1-hot encoding** \n",
    "- The extracted features are commonly encoded as **vectors** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Manual Image Feature Extraction\n",
    "\n",
    "![manual image feature extraction](./images/image-manual-feature-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Manual Text Feature Extraction\n",
    "\n",
    "![manual text feature extraction](./images/text-manual-feature-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üèπ Vectors\n",
    "\n",
    "- They are a **Numeric representation** of something in **N-dimensional** space\n",
    "- Can represent **anything**... entire documents, images, video, audio \n",
    "- Quantifies **features** or **characteristics** of the item\n",
    "- More importantly... they are **comparable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèπ Vectors\n",
    "\n",
    "- A Vector is a tuple of one or more **values** called **scalars**\n",
    "- Each **scalar** represents the measure of a **feature**\n",
    "- Different frameworks use different data types to represent them:\n",
    "  - In **Numpy** they are **Numpy Arrays** (`np.arrays`)\n",
    "  - In **TensorFlow** they are **Tensors** (`tf.Tensor`)\n",
    "  - In **PyTorch** they are also **Tensors** (`torch.tensor`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3 \"Bicycle Reviews\" Features as a Vector\n",
    "\n",
    "![represenation of a vector](./images/bicycle_vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üß® Issues with Feature Engineering\n",
    "\n",
    "- **Time-consuming**: Might require domain knowledge and expertise.\n",
    "- **High dimensionality**: Can lead to a high-dimensional feature space.\n",
    "- **Lack of scalability**: Not easily scalable, more data **==** more people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter \"Vector Embeddings\"\n",
    "\n",
    "- **Machine Learning** / **Deep Learning** have leaped forward in last decade \n",
    "- ML models **outperform** humans in many tasks nowadays\n",
    "  - üî• **CV** (Computer Vision) models excel at detection/classification\n",
    "  - üî• **LLMs** (Large Language Models) have advanced exponentially\n",
    "- Today, most vectors are **generated** using pre-trained **ML Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter \"Vector Embeddings\"\n",
    "\n",
    "- ML models can **extract contextual meaning** from unstructured data\n",
    "- Reduce semantically-rich high-dimensional inputs and **\"flatten\"** them \n",
    "- Flatten representations retain the semantic information and make for ideal vectors\n",
    "- Once in vector form the world of **linear algebra** allows to operate on vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector Embeddings from a CV Model\n",
    "\n",
    "![vector embedding extraction](./images/embedding-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Enter \"Vector Databases\"\n",
    "\n",
    "- Pure Vector Databases **efficiently store** Vectors (along with **metadata**)\n",
    "- Enable **searching** for vectors using **\"similarity\"** and **\"distance\"** metrics\n",
    "- Enable **hybrid searches** combining vectors and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redis as a Vector Database\n",
    "\n",
    "- Redis provides **Search Capabilities** for structured/semi-structured data\n",
    "- Redis supports `TEXT`, `NUMERIC`, `TAG`, `GEO` and `GEOSHAPE` fields\n",
    "- Redis introduces the **`VECTOR`** schema field type for vector support \n",
    "- **`VECTOR`** field allows **indexing**, and **querying** vectors in **Hashes** or **JSON**\n",
    "- Redis **in-memory** approach provides **fast** and **efficient** vector searches\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redis as a Vector Database\n",
    "\n",
    "- Capabilities:\n",
    "  - **3** distance metrics: **Euclidean**, **Internal Product** and **Cosine**\n",
    "  - **2** indexing methods: **HNSW** and **Flat**\n",
    "  - **Hybrid queries** combined with `GEO`, `TAG`, `TEXT` or `NUMERIC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ†Ô∏è Demo\n",
    "### The **Redis Fashion Store**\n",
    "\n",
    "![bikeshop](./images/dataset-cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The **Redis Fashion Store**\n",
    "\n",
    "* Populated from Kaggle's [**Fashion Product Images Dataset**](https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset)\n",
    "* Contains 44k products with descriptions, metadata, and images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connecting to Redis Stack\n",
    "\n",
    "* **Redis Stack** instance running locally\n",
    "* Import `redis-py` client library\n",
    "* Create a **client connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "client = redis.Redis(host='localhost', port=6379, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use the `PING` command to check that Redis is up and running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inspect the Store Items\n",
    "\n",
    "* Use the `JSON.GET` command to retrive the product with key `fashion:17445`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fashion17445 = client.json().get('fashion:17445')\n",
    "fashion17445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "client.json().objkeys('fashion:17445')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Embeddings with ML\n",
    "\n",
    "![ML Models for embeddings](./images/target-model-embeddings-redis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where to find pre-trained models?\n",
    "\n",
    "![Model Zoos](./images/model-zoos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "![SBERT](./images/sbert-net.png)\n",
    "\n",
    "- **SentenceTransformers** to **generate embeddings** for the product **descriptions** \n",
    "- **Sentence-BERT** (**SBERT**) produces **contextually rich** sentence embeddings\n",
    "- Embeddings provide **efficient sentence-level** semantic similarity\n",
    "- Improves tasks like **semantic search** and **text grouping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting a suitable pre-trained Model\n",
    "\n",
    "- We must pick a **suitable model** for **generating embeddings**\n",
    "- We want to query for products using **short queries** against the **longer** product **descriptions**\n",
    "- This is referred to as **\"Asymmetric Semantic Search\"** \n",
    "- Used when **search query** and the **documents** being searched are of **different nature or structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Selecting a suitable pre-trained Model\n",
    "\n",
    "- For **asymmetric semantic search** suitable models include pre-trained **MS MARCO** Models\n",
    "- Optimized for understanding **real-world queries** and producing **relevant responses**\n",
    "- **Highest performing** MS MARCO model is **`msmarco-distilbert-base-v4`**\n",
    "  - which is tuned for **cosine-similarity** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer('msmarco-distilbert-base-v4') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extract the Product's Description\n",
    "\n",
    "- Let's extract the `description` into the `sample_description` var:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_description = fashion17445['description']\n",
    "sample_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating an Embedding Vector\n",
    "\n",
    "- To generate the vector embeddings, we use the `encode` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedding = embedder.encode(sample_description)\n",
    "VECTOR_DIMENSION = len(embedding)\n",
    "VECTOR_DIMENSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's take a peek at the first **5** elements of the generated vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(embedding.tolist()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate Embeddings for remaining Products Descriptions\n",
    "\n",
    "* To vectorize all the descriptions in the database, we will first collect all the Redis keys for the products:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "all_keys = sorted(client.keys('fashion:*')) \n",
    "len(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import preso\n",
    "load_dotenv()\n",
    "DEMO_PRODUCTS = json.loads(os.environ.get('DEMO_PRODUCTS'))\n",
    "print(DEMO_PRODUCTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keys = [\"fashion:\" + str(id) for id in DEMO_PRODUCTS]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generate Embeddings for the Products Descriptions\n",
    "\n",
    "* With the keys in `keys` we can use the Redis `JSON.MGET` command to retrieve just the `description` field\n",
    "* We'll store all the descriptions in the `descriptions` variable\n",
    "* The `encode` method can take a List of text passages to encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "descriptions = client.json().mget(keys, '$.description')\n",
    "descriptions = [item for sublist in descriptions for item in sublist]\n",
    "embeddings = embedder.encode(descriptions).astype(np.float32).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let's checked that we've generated the correct number of embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Add the embeddings to the JSON documents\n",
    "\n",
    "- Now we can add the vectorized descriptions to the JSON documents in Redis\n",
    "- Use the `JSON.SET` command to insert a new field in each of the documents at `$.description_embeddings`\n",
    "- Use Redis' **pipeline** mode to minimize the round-trip times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "\n",
    "for key, embedding in zip(keys, embeddings):\n",
    "    pipeline.json().set(key, '$.description_embeddings', embedding)\n",
    "\n",
    "pipeline.execute()\n",
    "print('Vector Sentence Embeddings Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inspect the Products Documents\n",
    "\n",
    "- Let's inspect one of the vectorized product documents using the `JSON.GET` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(client.json().get('fashion:17445'), indent=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Create Search Index for the Products Collection\n",
    "\n",
    "- To define the index we'll import the `IndexDefinition` and the `IndexType`\n",
    "- To define the schema fields we'll use the classes `TagField`, `TextField`, `NumericField`, and **`VectorField`**\n",
    "- We'll create an index named **`idx:fashion`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.field import TagField, TextField, NumericField, VectorField\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "INDEX_NAME = 'idx:fashion'\n",
    "DOC_PREFIX = 'fashion:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Search Index Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.ft(INDEX_NAME).info()\n",
    "    print('Index already exists!')\n",
    "except:\n",
    "    schema = (\n",
    "        TagField('$.gender', as_name='gender'),  \n",
    "        TagField('$.subCategory', as_name='subCategory'), \n",
    "        TagField('$.season', as_name='season'), \n",
    "        NumericField('$.year', as_name='year'),\n",
    "        TagField('$.usage', as_name='usage'),\n",
    "        TextField('$.name', no_stem=True, as_name='name'),\n",
    "        TagField('$.color', as_name='color'),\n",
    "        TagField('$.type', as_name='type'),\n",
    "        TagField('$.category', as_name='category'),\n",
    "        TextField('$.description', as_name='description'),\n",
    "        VectorField('$.description_embeddings', 'FLAT', {\n",
    "          'TYPE': 'FLOAT32',\n",
    "          'DIM': VECTOR_DIMENSION,\n",
    "          'DISTANCE_METRIC': 'COSINE',\n",
    "        },  as_name='vector'),\n",
    "    )\n",
    "\n",
    "    # index Definition\n",
    "    definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.JSON)\n",
    "\n",
    "    # create Index\n",
    "    client.ft(INDEX_NAME).create_index(fields=schema, definition=definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `VECTOR` Schema Field Definition\n",
    "\n",
    "* **Indexing method**: `FLAT` **(brute-force indexing)** or `HNSW` **(Hierarchical Navigable Small World)**\n",
    "* **Vector Type**: `FLOAT32` or `FLOAT64`.\n",
    "* **Vector Dimension**: The length or dimension of our embeddings (`768`).\n",
    "* **Distance Metric**: `L2` **(Euclidean distance)**, `IP` **(Inner Product)**, or `COSINE` **(Cosine Similarity)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Check the state of the Index\n",
    "\n",
    "- `FT.CREATE` creates the index\n",
    "- The **indexing process** is automatically started in the **background**\n",
    "- In the blink of an eye, our JSON documents are indexed and ready to be searched\n",
    "- To corroborate that, we use the **`FT.INFO`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "info = client.ft(INDEX_NAME).info()\n",
    "\n",
    "num_docs = info['num_docs']\n",
    "indexing_failures = info['hash_indexing_failures']\n",
    "total_indexing_time = info['total_indexing_time']\n",
    "percent_indexed = float(info['percent_indexed']) * 100\n",
    "\n",
    "\n",
    "print(f\"{num_docs} docs ({percent_indexed}%) indexed w/ {indexing_failures} failures in {float(total_indexing_time):.2f} msecs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Structured Data Searches with Redis\n",
    "\n",
    "- Let's test the non-vector part of the index first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Retrieve all products where the `type` is `Deodorant`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('@type:{Deodorant}')\n",
    "    .paging(0, 5)\n",
    "    .return_fields('id', 'name', 'color', 'type', 'year')\n",
    ")\n",
    "client.ft(INDEX_NAME).search(query).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Find all `Deodorant` products find the ones with the word `hydra` in their `name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('@type:{Deodorant} @name:hydra').return_fields('id', 'name', 'color', 'type', 'year')\n",
    ")\n",
    "client.ft(INDEX_NAME).search(query).docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Semantic Queries\n",
    "\n",
    "- We want to query for products using short query prompts\n",
    "- Let's put our queries in a list so we can vectorize them and execute them in bulk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    'Comfortable pants',\n",
    "    'Casual shades',\n",
    "    'Floral dress'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "encoded_queries = embedder.encode(queries)\n",
    "len(encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "- TensorFlow provides \"projector\" with some sample vector spaces mapped into a 3-D space\n",
    "- By using Dimensionality Reduction techniques we can visualize and explore embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment",
     "tags": [
      "hide-input"
     ]
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://projector.tensorflow.org/\" width=\"1920\" height=\"540\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a \"Pure KNN\" VSS Query\n",
    "\n",
    "- We'll start with a **K-nearest neighbors** (KNN) query \n",
    "- KNN goal is to find the **most similar** items to a given query item\n",
    "- KNN calculates the **distance** between the query vector and each vector in the database\n",
    "- Returns 'K' items with the **smallest** distances\n",
    "- These are considered to be the most similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Constructing a \"Pure KNN\" VSS Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('(*)=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'name', 'color', 'type', 'year', 'description')\n",
    "     .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The syntax for KNN queries is `(*)=>[vector_similarity_query>]` \n",
    "  - where the `(*)` (the `*` meaning all) is the filter query for the search engine.\n",
    "  - `$query_vector` represents the query parameter we'll use to pass the vectorized query prompt.\n",
    "  - results are filtered by `vector_score`\n",
    "  - Query returns the `vector_score`, the `id` of the matched documents, the `$.brand`, `$.model`, and `$.description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query\n",
    "\n",
    "- With the Query prepared in `query`\n",
    "- and the query prompts in `queries` \n",
    "- and the encoded queries in `encoded_queries`\n",
    "- we can use the `create_query_table` function to generate a table of results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preso.create_query_table(client, INDEX_NAME, query, queries, encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üîç Pre-filtering Queries\n",
    "\n",
    "- \"Pure KNN\" queries evaluate a query against the **whole space of vectors**\n",
    "- The larger the collection, the more **computationally expensive**\n",
    "- Unstructured data does not live in isolation\n",
    "- Rich search experiences must allow searching all data (structured and unstructured) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üîç Pre-filtering Queries\n",
    "\n",
    "- For example, users might arrive at your search interface with a brand preference in mind\n",
    "- Redis can use this information to pre-filter the search space\n",
    "- In the hybrid query definition below:\n",
    "  - we pre-filter using the `color` equal to `Blue` and the `season` being `Summer` \n",
    "  - before our primary filter query was `(*)`, AKA everything\n",
    "  - we narrow the search space using `(@brand:Peaknetic)` before the KNN query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hybrid_query = (\n",
    "    Query('(@color:{Blue} @season:{Summer})=>[KNN 3 @vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score', 'id', 'name', 'color', 'type', 'year', 'description')\n",
    "     .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preso.create_query_table(client, INDEX_NAME, hybrid_query, queries, encoded_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Range Queries\n",
    "\n",
    "- Range queries retrieve items within a specific **distance** from a query vector\n",
    "- We consider **\"distance\"** to be the **measure of similarity** \n",
    "- The smaller the distance, the more similar the items\n",
    "- For example, to return the top `4` products within a `0.55` \"radius\" of query: \n",
    "\n",
    "```\n",
    "1Ô∏è‚É£ FT.SEARCH idx:fashion \n",
    "2Ô∏è‚É£   @vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: vector_score} \n",
    "3Ô∏è‚É£   SORTBY vector_score ASC\n",
    "4Ô∏è‚É£   LIMIT 0 4 \n",
    "5Ô∏è‚É£   DIALECT 2 \n",
    "6Ô∏è‚É£   PARAMS 4 range 0.55 query_vector \"\\x9d|\\x99>bV#\\xbfm\\x86\\x8a\\xbd\\xa7~$?*....\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Range Queries\n",
    "\n",
    "- In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "range_query = (\n",
    "    Query('@vector:[VECTOR_RANGE $range $query_vector]=>{$YIELD_DISTANCE_AS: vector_score}') \n",
    "    .sort_by('vector_score')\n",
    "    .return_fields('vector_score', 'id', 'name', 'color', 'type', 'year', 'description')\n",
    "    .paging(0, 4)\n",
    "    .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üèÉüèæ‚Äç‚ôÄÔ∏èRunning the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preso.create_query_table(client, INDEX_NAME, range_query, queries, encoded_queries, {'range': 0.75})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üì∏ Searching using Similar Images\n",
    "\n",
    "* Just like we did we the text descriptions, we can use the images associated with the products\n",
    "* The \"Sometimes a picture is word 1000 words\" adage applies well to searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "sample_image = Image.open(fashion17445['image_url'])\n",
    "sample_image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## üì∏ Finding an Image Embedding Model\n",
    "\n",
    "- For generating Image Embeddings I've chosen the CLIP model\n",
    "- Specifically the `clip-ViT-B-32` which maps text and images to a shared vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_embedder = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "sample_image_224x224 = sample_image.convert('RGB').resize((224, 224))\n",
    "\n",
    "image_embedding = image_embedder.encode(sample_image_224x224)\n",
    "IMAGE_VECTOR_DIMENSION = len(image_embedding)\n",
    "IMAGE_VECTOR_DIMENSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(image_embedding.tolist()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  üì∏ Generate the Image Embeddings for the Demo Products\n",
    "\n",
    "- As before, we use the `encode` function over the collection of products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_urls = client.json().mget(keys, '$.image_url')\n",
    "image_embeddings = []\n",
    "\n",
    "for filepath, key in zip(image_urls, keys):    \n",
    "    image = Image.open(filepath[0]).convert('RGB')\n",
    "    image = image.resize((224, 224))\n",
    "    image_embeddings.append(image_embedder.encode(image).astype(np.float32).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ü™£ Store the Embeddings in Redis\n",
    "\n",
    "* Add the new `image_embeddings` field to the JSON documents\n",
    "* Again we use Redis \"pipeline\" mode to minimize the network trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "\n",
    "for key, image_embedding in zip(keys, image_embeddings):\n",
    "    pipeline.json().set(key, '$.image_embeddings', image_embedding)\n",
    "\n",
    "pipeline.execute()\n",
    "print('Vector Image Embeddings Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Update the Search Index\n",
    "\n",
    "* To enable searching against the created image embeddings we alter the schema\n",
    "* Redis provides the [FT.ALTER](https://redis.io/commands/ft.alter/) command to do so\n",
    "* Available in Redis PY as `alter_schema_add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "client.ft(INDEX_NAME).alter_schema_add(\n",
    "    VectorField('$.image_embeddings', 'FLAT', {\n",
    "      'TYPE': 'FLOAT32',\n",
    "      'DIM': IMAGE_VECTOR_DIMENSION,\n",
    "      'DISTANCE_METRIC': 'COSINE',\n",
    "    },  as_name='image_vector')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finding a \"Query Image\"\n",
    "\n",
    "* Let's pick a random image from database to search for similar items visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(image_urls) - 1)\n",
    "query_image = Image.open(image_urls[random_index][0])\n",
    "query_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image Similarity Vector Query\n",
    "\n",
    "* As before, we'll use a KNN query but with K=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_query = (\n",
    "    Query('(*)=>[KNN 7 @image_vector $query_vector AS image_vector_score]')\n",
    "     .sort_by('image_vector_score')\n",
    "     .return_fields('image_vector_score', 'id')\n",
    "     .paging(1, 6)\n",
    "     .dialect(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running the Query\n",
    "\n",
    "* We run the query by \"vectorizing\" the query image...\n",
    "* The we grab the images corresponding to the matched documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "results = client.ft(INDEX_NAME).search(image_query, { 'query_vector': np.array(image_embeddings[random_index], dtype=np.float32).tobytes() }).docs\n",
    "\n",
    "image_matches_urls = client.json().mget([doc.id for doc in results], '$.image_url')\n",
    "image_matches = [Image.open(url[0]) for url in image_matches_urls] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizing the Results\n",
    "\n",
    "* Since we're in a Jupyter Notebook, let's use MatplotLib to show a grid out our visual matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "preso.plot_images(2, 3, image_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- The tools and techniques to unlock the value in **Unstructured Data** have evolved greatly...\n",
    "- Redis **in-memory first** approach makes it a perfect fit for vector similarity searches\n",
    "- Redis natively supports vector searches over **Hashes** and **JSON**\n",
    "- Redis combines the power of searching over semi-structured and unstructured data\n",
    "  - with the performance you've come to expect from Redis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## https://github.com/bsbodden/redis-vss-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learn more at Redis University\n",
    "\n",
    "## `https://university.redis.com`\n",
    "\n",
    "![Redis U](./images/redis_university.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank You!\n",
    "\n",
    "![BSB](./images/bsb.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "rise": {}
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
